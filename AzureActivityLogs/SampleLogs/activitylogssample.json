      [
      {
        "caller": "AcmClient@microsoft.com",
        "channels": "Operation",
        "claims": {
          "http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress": "AcmClient@microsoft.com"
        },
        "correlationId": "9d0133c3-c8b7-4e6e-8a83-ade1ea7ab9fa",
        "description": "Resolved: PIR/Invite - Azure Monitor - Logs data access issues",
        "eventDataId": "3d37f853-ecb9-d334-d121-7800dc3d622c",
        "eventName": {
          "value": "",
          "localizedValue": ""
        },
        "category": {
          "value": "ServiceHealth",
          "localizedValue": "Service Health"
        },
        "id": "/subscriptions/5044ad67-cc18-4153-be7e-cefd01e28625/events/3d37f853-ecb9-d334-d121-7800dc3d622c/ticks/638270517325244843",
        "level": "Warning",
        "resourceGroupName": "",
        "resourceProviderName": {
          "value": "",
          "localizedValue": ""
        },
        "resourceId": "/subscriptions/5044ad67-cc18-4153-be7e-cefd01e28625",
        "resourceType": {
          "value": "",
          "localizedValue": ""
        },
        "operationId": "",
        "operationName": {
          "value": "Microsoft.ServiceHealth/incident/action",
          "localizedValue": "Microsoft.ServiceHealth/incident/action"
        },
        "properties": {
          "title": "PIR/Invite - Azure Monitor - Logs data access issues",
          "service": "Alerts",
          "region": "Global",
          "communication": "<p><strong>Post Incident Review (PIR) – Azure Monitor – Logs data access issues</strong></p>\n<p><br></p>\n<p><em>This incident will be discussed as part of an upcoming Azure Incident Retrospective customer livestream webcast event.</em></p>\n<p><em>Join either session to hear from our engineering leaders - on what happened, how we responded, and what we learned.</em></p>\n<p><em>The event includes a live Q&amp;A experience, to ask our engineering experts any questions you have about the incident.</em></p>\n<p><em>Session option #1 - 15 August 2023 @ 15:00 UTC - Register at:&nbsp;</em><a href=\"https://www.aka.ms/air5/reg/ash/1of2\" target=\"_blank\"><em>https://www.aka.ms/air5/reg/ash/1of2</em></a></p>\n<p><em>Session option #2 - 16 August 2023 @ 02:00 UTC - Register at:&nbsp;</em><a href=\"https://www.aka.ms/air5/reg/ash/2of2\" target=\"_blank\"><em>https://www.aka.ms/air5/reg/ash/2of2</em></a></p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p>Between 23:15 UTC on 6 July 2023 and 09:00 UTC on 7 July 2023, a subset of data for Azure Monitor Log Analytics and Microsoft Sentinel failed to ingest. Additionally, platform logs gathered via Diagnostic Settings failed to route some data to customer destinations such as Log Analytics, Storage, Event Hub and Marketplace. These failures were caused by a deployment of a service within Microsoft, with a bug that caused a much higher than expected call volume that overwhelmed the telemetry management control plane. Customers in all regions experienced impact. Security Operations Center (SOC) functionality in Sentinel including hunting queries, workbooks with custom queries, and notebooks that queried impacted tables with date range inclusive of the deleted log data might have returned partial or empty results. In cases where Event or Security Event tables were impacted, incident investigations of a correlated incident may have showed partial or empty results. Unfortunately, this issue impacted one or more of your Azure resources. We’re providing you with this Post Incident Review (PIR) to summarize what went wrong, how we responded, and the steps Microsoft is taking to learn and improve.</p>\n<p><br></p>\n<p><strong>What went wrong and why?</strong></p>\n<p>A code deployment for the Azure Container Apps service was started on 3 July 2023 via the normal Safe Deployment Practices (SDP), first rolling out to Azure canary and staging regions. This version contained a misconfiguration that blocked the service from starting normally. Due to the misconfiguration, the service bootstrap code threw an exception, and was automatically restarted. This caused the bootstrap service to be stuck in a loop where it was being restarted every 5 to 10 seconds. Each time the bootstrap service was restarted, it provided configuration information to the telemetry agents also installed on the service hosts. Each time the configuration information was sent to the telemetry hosts, they interpreted this as a configuration change, and therefore they also automatically exited their current process and restarted as well. Three separate instances of the agent telemetry host, per application host, were now also restarting every 5 to 10 seconds.</p>\n<p>Upon each startup of the telemetry agent, the agent immediately contacted the telemetry control plane to download the latest version of the telemetry configuration. Normally this is an action that would take place one time every several days, as this configuration would be cached on the agent. However, as the deployment of the Container Apps service progressed, several hundred hosts now had their telemetry agents requesting startup configuration information from the telemetry control plane every 5-10 seconds. The Container Apps team detected the fault in their deployment on 6 July 2023, stopped the original deployment before it was released to any production regions, and started a new deployment of their service in the canary and staging regions to correct the misconfiguration.</p>\n<p>However, the aggregate rate of requests from the services that received the build with the misconfiguration exhausted capacity on the telemetry control plane. The telemetry control plane is a global service, used by services running in all public regions of Azure. As capacity on the control plane was saturated, other services involved in ingestion of telemetry, such as the ingestion front doors and the pipeline services that route data between services internally, began to fail as their operations against the telemetry control plane were either rejected or timed out. The design of the telemetry control plane as a single point of failure is a known risk, and investment to eliminate this risk has been underway in Azure Monitor to design this risk out of the system.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p>The impact on the telemetry control plane grew slowly and did not create problems that were detected until 12:30 UTC on 6 July 2023. When the issues were detected, the source of the additional load against the telemetry control plane was not known, but the team suspected additional load had been created against the control plane and took these actions:</p>\n<ul><li>6 July 2023 @ 14:53 UTC – Internal incident bridge created</li><li>6 July 2023 @ 15:56 UTC – ~500 instances of garbage collector service were removed, to reduce load on telemetry control plane</li><li>6 July 2023 @ 16:09 UTC – First batch of Node Diagnostics servers were removed, to reduce load on telemetry control plane. This process of removing this type of server continued over the next 10 hours.</li><li>6 July 2023 @ 19:19 UTC – We sent initial customer notifications via Azure Service Health to customers with impacted subscriptions (sent on Tracking ID: XMGF-5Z0).</li><li>6 July 2023 @ 20:20 UTC – Source of anomalously high traffic was identified, and the responsible team was paged to assist.</li><li>6 July 2023 @ 23:00 UTC – IP address blocks deployed, to prevent anomalous traffic from hitting telemetry control plane</li><li>7 July 2023 @ 01:30 UTC – Three additional clusters were added to telemetry control plane to handle additional load, and we began restarting existing clusters to clear backlogged connections</li><li>7 July 2023 @ 02:45 UTC – An additional three clusters were added to telemetry control plane</li><li>7 July 2023 @ 05:21 UTC – A more detailed customer update notification was sent</li><li>7 July 2023 @ 09:00 UTC – Incident declared mitigated, as call error rate and call latency against control plane APIs stabilized at typical levels.</li></ul>\n<p><br></p>\n<p><strong>How are we making incidents like this less likely or less impactful?</strong></p>\n<p></p><ul><li>We know customer trust is earned and must be maintained, not just by saying the right thing but by doing the right thing. Data retention is a fundamental responsibility of the Microsoft cloud, including every engineer working on every cloud service. We have learned from this incident and are committed to the following improvements:</li><li>We have ensured that our telemetry control plane services are now running with additional capacity (Completed)</li><li>We are creating additional alerting on certain metrics that indicate critical, unusual failure patterns in API calls (Estimated completion: July 2023)</li><li>We will be adding new positive caching and negative caching to the control plane, to reduce load on backing store (Estimated completion: September 2023)</li><li>We are putting in place additional throttling and circuit breaker patterns to our core telemetry control plane APIs (Estimated completion: September 2023)</li><li>In the longer term, we are creating isolation between internal-facing and external-facing services using the telemetry control plane (Estimated completion: December 2023)</li></ul><p><br></p><p><strong>How can customers make incidents like this less impactful?</strong></p><p>Customers did not have options available to them to prevent impact from this incident, or to mitigate impact during the incident.</p><p>More generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review: <a href=\"https://docs.microsoft.com/en-us/azure/architecture/framework/resiliency\" target=\"_blank\">https://docs.microsoft.com/en-us/azure/architecture/framework/resiliency</a></p><p>Finally, consider ensuring that the right people in your organization will be notified about any future service issues - by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:<a href=\"https://aka.ms/ash-alerts\" target=\"_blank\"> https://aka.ms/ash-alerts</a></p><p><br></p><p><strong>How can we make our incident communications more useful?</strong></p><p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22PIRversion%22:%22V1%22,%22TrackingID%22:%22XMGF-5Z0%22%7D\" target=\"_blank\">3-question survey</a></p><p></p>",
          "incidentType": "Incident",
          "trackingId": "XMGF-5Z0",
          "impactStartTime": "7/5/2023 10:00:00 PM",
          "impactMitigationTime": "7/7/2023 10:25:11 AM",
          "impactedServices": "[{\"ImpactedRegions\":[{\"RegionName\":\"Global\",\"RegionId\":\"global\"}],\"ServiceName\":\"Alerts\",\"ServiceId\":\"alerts\"},{\"ImpactedRegions\":[{\"RegionName\":\"Global\",\"RegionId\":\"global\"}],\"ServiceName\":\"Diagnostic Logs\",\"ServiceId\":\"diagnosticlogs\"},{\"ImpactedRegions\":[{\"RegionName\":\"Australia Central\",\"RegionId\":\"australiacentral\"},{\"RegionName\":\"Australia Central 2\",\"RegionId\":\"australiacentral2\"},{\"RegionName\":\"Australia East\",\"RegionId\":\"australiaeast\"},{\"RegionName\":\"Australia Southeast\",\"RegionId\":\"australiasoutheast\"},{\"RegionName\":\"Brazil South\",\"RegionId\":\"brazilsouth\"},{\"RegionName\":\"Brazil Southeast\",\"RegionId\":\"brazilsoutheast\"},{\"RegionName\":\"Canada Central\",\"RegionId\":\"canadacentral\"},{\"RegionName\":\"Canada East\",\"RegionId\":\"canadaeast\"},{\"RegionName\":\"Central India\",\"RegionId\":\"centralindia\"},{\"RegionName\":\"Central US\",\"RegionId\":\"centralus\"},{\"RegionName\":\"Central US EUAP\",\"RegionId\":\"centraluseuap\"},{\"RegionName\":\"East Asia\",\"RegionId\":\"eastasia\"},{\"RegionName\":\"East US\",\"RegionId\":\"eastus\"},{\"RegionName\":\"East US 2\",\"RegionId\":\"eastus2\"},{\"RegionName\":\"East US 2 EUAP\",\"RegionId\":\"eastus2euap\"},{\"RegionName\":\"France Central\",\"RegionId\":\"francecentral\"},{\"RegionName\":\"France South\",\"RegionId\":\"francesouth\"},{\"RegionName\":\"Germany North\",\"RegionId\":\"germanynorth\"},{\"RegionName\":\"Germany West Central\",\"RegionId\":\"germanywestcentral\"},{\"RegionName\":\"Japan East\",\"RegionId\":\"japaneast\"},{\"RegionName\":\"Japan West\",\"RegionId\":\"japanwest\"},{\"RegionName\":\"Jio India Central\",\"RegionId\":\"jioindiacentral\"},{\"RegionName\":\"Jio India West\",\"RegionId\":\"jioindiawest\"},{\"RegionName\":\"Korea Central\",\"RegionId\":\"koreacentral\"},{\"RegionName\":\"Korea South\",\"RegionId\":\"koreasouth\"},{\"RegionName\":\"North Central US\",\"RegionId\":\"northcentralus\"},{\"RegionName\":\"North Europe\",\"RegionId\":\"northeurope\"},{\"RegionName\":\"Norway East\",\"RegionId\":\"norwayeast\"},{\"RegionName\":\"Norway West\",\"RegionId\":\"norwaywest\"},{\"RegionName\":\"Qatar Central\",\"RegionId\":\"qatarcentral\"},{\"RegionName\":\"South Africa North\",\"RegionId\":\"southafricanorth\"},{\"RegionName\":\"South Africa West\",\"RegionId\":\"southafricawest\"},{\"RegionName\":\"South Central US\",\"RegionId\":\"southcentralus\"},{\"RegionName\":\"South India\",\"RegionId\":\"southindia\"},{\"RegionName\":\"Southeast Asia\",\"RegionId\":\"southeastasia\"},{\"RegionName\":\"Sweden Central\",\"RegionId\":\"swedencentral\"},{\"RegionName\":\"Sweden South\",\"RegionId\":\"swedensouth\"},{\"RegionName\":\"Switzerland North\",\"RegionId\":\"switzerlandnorth\"},{\"RegionName\":\"Switzerland West\",\"RegionId\":\"switzerlandwest\"},{\"RegionName\":\"UK South\",\"RegionId\":\"uksouth\"},{\"RegionName\":\"UK West\",\"RegionId\":\"ukwest\"},{\"RegionName\":\"West Central US\",\"RegionId\":\"westcentralus\"},{\"RegionName\":\"West Europe\",\"RegionId\":\"westeurope\"},{\"RegionName\":\"West India\",\"RegionId\":\"westindia\"},{\"RegionName\":\"West US\",\"RegionId\":\"westus\"},{\"RegionName\":\"West US 2\",\"RegionId\":\"westus2\"},{\"RegionName\":\"West US 3\",\"RegionId\":\"westus3\"}],\"ServiceName\":\"Log Analytics\",\"ServiceId\":\"loganalytics\"}]",
          "impactedServicesTableRows": "<tr>\r\n<td align='center' style='padding: 5px 10px; border-right:1px solid black; border-bottom:1px solid black'>Alerts</td>\r\n<td align='center' style='padding: 5px 10px; border-bottom:1px solid black'>Global<br></td>\r\n</tr>\r\n<tr>\r\n<td align='center' style='padding: 5px 10px; border-right:1px solid black; border-bottom:1px solid black'>Diagnostic Logs</td>\r\n<td align='center' style='padding: 5px 10px; border-bottom:1px solid black'>Global<br></td>\r\n</tr>\r\n<tr>\r\n<td align='center' style='padding: 5px 10px; border-right:1px solid black; border-bottom:1px solid black'>Log Analytics</td>\r\n<td align='center' style='padding: 5px 10px; border-bottom:1px solid black'>Australia Central<br>Australia Central 2<br>Australia East<br>Australia Southeast<br>Brazil South<br>Brazil Southeast<br>Canada Central<br>Canada East<br>Central India<br>Central US<br>Central US EUAP<br>East Asia<br>East US<br>East US 2<br>East US 2 EUAP<br>France Central<br>France South<br>Germany North<br>Germany West Central<br>Japan East<br>Japan West<br>Jio India Central<br>Jio India West<br>Korea Central<br>Korea South<br>North Central US<br>North Europe<br>Norway East<br>Norway West<br>Qatar Central<br>South Africa North<br>South Africa West<br>South Central US<br>South India<br>Southeast Asia<br>Sweden Central<br>Sweden South<br>Switzerland North<br>Switzerland West<br>UK South<br>UK West<br>West Central US<br>West Europe<br>West India<br>West US<br>West US 2<br>West US 3<br></td>\r\n</tr>\r\n",
          "defaultLanguageTitle": "PIR/Invite - Azure Monitor - Logs data access issues",
          "defaultLanguageContent": "<p><strong>Post Incident Review (PIR) – Azure Monitor – Logs data access issues</strong></p>\n<p><br></p>\n<p><em>This incident will be discussed as part of an upcoming Azure Incident Retrospective customer livestream webcast event.</em></p>\n<p><em>Join either session to hear from our engineering leaders - on what happened, how we responded, and what we learned.</em></p>\n<p><em>The event includes a live Q&amp;A experience, to ask our engineering experts any questions you have about the incident.</em></p>\n<p><em>Session option #1 - 15 August 2023 @ 15:00 UTC - Register at:&nbsp;</em><a href=\"https://www.aka.ms/air5/reg/ash/1of2\" target=\"_blank\"><em>https://www.aka.ms/air5/reg/ash/1of2</em></a></p>\n<p><em>Session option #2 - 16 August 2023 @ 02:00 UTC - Register at:&nbsp;</em><a href=\"https://www.aka.ms/air5/reg/ash/2of2\" target=\"_blank\"><em>https://www.aka.ms/air5/reg/ash/2of2</em></a></p>\n<p><br></p>\n<p><strong>What happened?</strong></p>\n<p>Between 23:15 UTC on 6 July 2023 and 09:00 UTC on 7 July 2023, a subset of data for Azure Monitor Log Analytics and Microsoft Sentinel failed to ingest. Additionally, platform logs gathered via Diagnostic Settings failed to route some data to customer destinations such as Log Analytics, Storage, Event Hub and Marketplace. These failures were caused by a deployment of a service within Microsoft, with a bug that caused a much higher than expected call volume that overwhelmed the telemetry management control plane. Customers in all regions experienced impact. Security Operations Center (SOC) functionality in Sentinel including hunting queries, workbooks with custom queries, and notebooks that queried impacted tables with date range inclusive of the deleted log data might have returned partial or empty results. In cases where Event or Security Event tables were impacted, incident investigations of a correlated incident may have showed partial or empty results. Unfortunately, this issue impacted one or more of your Azure resources. We’re providing you with this Post Incident Review (PIR) to summarize what went wrong, how we responded, and the steps Microsoft is taking to learn and improve.</p>\n<p><br></p>\n<p><strong>What went wrong and why?</strong></p>\n<p>A code deployment for the Azure Container Apps service was started on 3 July 2023 via the normal Safe Deployment Practices (SDP), first rolling out to Azure canary and staging regions. This version contained a misconfiguration that blocked the service from starting normally. Due to the misconfiguration, the service bootstrap code threw an exception, and was automatically restarted. This caused the bootstrap service to be stuck in a loop where it was being restarted every 5 to 10 seconds. Each time the bootstrap service was restarted, it provided configuration information to the telemetry agents also installed on the service hosts. Each time the configuration information was sent to the telemetry hosts, they interpreted this as a configuration change, and therefore they also automatically exited their current process and restarted as well. Three separate instances of the agent telemetry host, per application host, were now also restarting every 5 to 10 seconds.</p>\n<p>Upon each startup of the telemetry agent, the agent immediately contacted the telemetry control plane to download the latest version of the telemetry configuration. Normally this is an action that would take place one time every several days, as this configuration would be cached on the agent. However, as the deployment of the Container Apps service progressed, several hundred hosts now had their telemetry agents requesting startup configuration information from the telemetry control plane every 5-10 seconds. The Container Apps team detected the fault in their deployment on 6 July 2023, stopped the original deployment before it was released to any production regions, and started a new deployment of their service in the canary and staging regions to correct the misconfiguration.</p>\n<p>However, the aggregate rate of requests from the services that received the build with the misconfiguration exhausted capacity on the telemetry control plane. The telemetry control plane is a global service, used by services running in all public regions of Azure. As capacity on the control plane was saturated, other services involved in ingestion of telemetry, such as the ingestion front doors and the pipeline services that route data between services internally, began to fail as their operations against the telemetry control plane were either rejected or timed out. The design of the telemetry control plane as a single point of failure is a known risk, and investment to eliminate this risk has been underway in Azure Monitor to design this risk out of the system.</p>\n<p><br></p>\n<p><strong>How did we respond?</strong></p>\n<p>The impact on the telemetry control plane grew slowly and did not create problems that were detected until 12:30 UTC on 6 July 2023. When the issues were detected, the source of the additional load against the telemetry control plane was not known, but the team suspected additional load had been created against the control plane and took these actions:</p>\n<ul><li>6 July 2023 @ 14:53 UTC – Internal incident bridge created</li><li>6 July 2023 @ 15:56 UTC – ~500 instances of garbage collector service were removed, to reduce load on telemetry control plane</li><li>6 July 2023 @ 16:09 UTC – First batch of Node Diagnostics servers were removed, to reduce load on telemetry control plane. This process of removing this type of server continued over the next 10 hours.</li><li>6 July 2023 @ 19:19 UTC – We sent initial customer notifications via Azure Service Health to customers with impacted subscriptions (sent on Tracking ID: XMGF-5Z0).</li><li>6 July 2023 @ 20:20 UTC – Source of anomalously high traffic was identified, and the responsible team was paged to assist.</li><li>6 July 2023 @ 23:00 UTC – IP address blocks deployed, to prevent anomalous traffic from hitting telemetry control plane</li><li>7 July 2023 @ 01:30 UTC – Three additional clusters were added to telemetry control plane to handle additional load, and we began restarting existing clusters to clear backlogged connections</li><li>7 July 2023 @ 02:45 UTC – An additional three clusters were added to telemetry control plane</li><li>7 July 2023 @ 05:21 UTC – A more detailed customer update notification was sent</li><li>7 July 2023 @ 09:00 UTC – Incident declared mitigated, as call error rate and call latency against control plane APIs stabilized at typical levels.</li></ul>\n<p><br></p>\n<p><strong>How are we making incidents like this less likely or less impactful?</strong></p>\n<p></p><ul><li>We know customer trust is earned and must be maintained, not just by saying the right thing but by doing the right thing. Data retention is a fundamental responsibility of the Microsoft cloud, including every engineer working on every cloud service. We have learned from this incident and are committed to the following improvements:</li><li>We have ensured that our telemetry control plane services are now running with additional capacity (Completed)</li><li>We are creating additional alerting on certain metrics that indicate critical, unusual failure patterns in API calls (Estimated completion: July 2023)</li><li>We will be adding new positive caching and negative caching to the control plane, to reduce load on backing store (Estimated completion: September 2023)</li><li>We are putting in place additional throttling and circuit breaker patterns to our core telemetry control plane APIs (Estimated completion: September 2023)</li><li>In the longer term, we are creating isolation between internal-facing and external-facing services using the telemetry control plane (Estimated completion: December 2023)</li></ul><p><br></p><p><strong>How can customers make incidents like this less impactful?</strong></p><p>Customers did not have options available to them to prevent impact from this incident, or to mitigate impact during the incident.</p><p>More generally, consider evaluating the reliability of your applications using guidance from the Azure Well-Architected Framework and its interactive Well-Architected Review: <a href=\"https://docs.microsoft.com/en-us/azure/architecture/framework/resiliency\" target=\"_blank\">https://docs.microsoft.com/en-us/azure/architecture/framework/resiliency</a></p><p>Finally, consider ensuring that the right people in your organization will be notified about any future service issues - by configuring Azure Service Health alerts. These can trigger emails, SMS, push notifications, webhooks, and more:<a href=\"https://aka.ms/ash-alerts\" target=\"_blank\"> https://aka.ms/ash-alerts</a></p><p><br></p><p><strong>How can we make our incident communications more useful?</strong></p><p>You can rate this PIR and provide any feedback using our quick <a href=\"https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxYsbHJUQAdChwAJoetCuW5UQlUxQlhaQjQ4SlNZOUs1WDZMMFNDWlBORyQlQCN0PWcu&amp;ctx=%7B%22PIRversion%22:%22V1%22,%22TrackingID%22:%22XMGF-5Z0%22%7D\" target=\"_blank\">3-question survey</a></p><p></p>",
          "stage": "RCA",
          "communicationId": "11000122779813",
          "maintenanceId": "",
          "maintenanceType": "",
          "isHIR": "False",
          "IsSynthetic": "False",
          "impactType": "SubscriptionList",
          "emailTemplateId": "tem_jCfXGkcx9fMD8p4hq6XVdGv9",
          "emailTemplateFullVersion": "ver_YmZvWy6k7MwmcYj3KYg8MloA",
          "emailTemplateLocale": "en-US",
          "smsText": "Resolved:PIR/Invite - Azure Monitor - Logs data access issues",
          "communicationRouteType": "Normal",
          "version": "0.1.1",
          "argQuery": ""
        },
        "status": {
          "value": "Resolved",
          "localizedValue": "Resolved"
        },
        "subStatus": {
          "value": "",
          "localizedValue": ""
        },
        "eventTimestamp": "2023-08-08T00:35:32.5244843Z",
        "submissionTimestamp": "2023-08-08T00:35:32.5244843Z",
        "subscriptionId": "5044ad67-cc18-4153-be7e-cefd01e28625",
        "tenantId": ""
      },
      {
        "channels": "Operation",
        "correlationId": "37abbfff-f5b3-476f-9040-20c6e29d4cf1",
        "description": "CorrelationId: 37abbfff-f5b3-476f-9040-20c6e29d4cf1\r\nTimestamp: 2023-08-07T23:10:02.8516588Z\r\nCreating temp folder.\r\nRetrieve site meta-data.\r\nBacking up the databases.\r\nError in the backup operation: The website + database size exceeds the 10 GB limit for backups. Your content size is 10 GB. Try using a backup.filter file to exclude some files from the backup as described here https://aka.ms/partial-backup, or try removing the database portion of the backup and use externally offered database backups instead.\r\n",
        "eventDataId": "3d7b466a-1cf7-6c28-7dd4-95c96f659182",
        "eventName": {
          "value": "",
          "localizedValue": ""
        },
        "category": {
          "value": "Administrative",
          "localizedValue": "Administrative"
        },
        "id": "/SUBSCRIPTIONS/5044AD67-CC18-4153-BE7E-CEFD01E28625/RESOURCEGROUPS/CASAMENTORG/PROVIDERS/MICROSOFT.WEB/SITES/JOANAEPEDRO/events/3d7b466a-1cf7-6c28-7dd4-95c96f659182/ticks/638270466120000000",
        "level": "Informational",
        "resourceGroupName": "CASAMENTORG",
        "resourceProviderName": {
          "value": "MICROSOFT.WEB",
          "localizedValue": "Azure Web Sites"
        },
        "resourceId": "/SUBSCRIPTIONS/5044AD67-CC18-4153-BE7E-CEFD01E28625/RESOURCEGROUPS/CASAMENTORG/PROVIDERS/MICROSOFT.WEB/SITES/JOANAEPEDRO",
        "resourceType": {
          "value": "MICROSOFT.WEB/sites",
          "localizedValue": "MICROSOFT.WEB/sites"
        },
        "operationId": "",
        "operationName": {
          "value": "BackupWebSite",
          "localizedValue": "BackupWebSite"
        },
        "properties": {
          "Message": "CorrelationId: 37abbfff-f5b3-476f-9040-20c6e29d4cf1\r\nTimestamp: 2023-08-07T23:10:02.8516588Z\r\nCreating temp folder.\r\nRetrieve site meta-data.\r\nBacking up the databases.\r\nError in the backup operation: The website + database size exceeds the 10 GB limit for backups. Your content size is 10 GB. Try using a backup.filter file to exclude some files from the backup as described here https://aka.ms/partial-backup, or try removing the database portion of the backup and use externally offered database backups instead.\r\n"
        },
        "status": {
          "value": "Failed",
          "localizedValue": "Failed"
        },
        "subStatus": {
          "value": "",
          "localizedValue": ""
        },
        "eventTimestamp": "2023-08-07T23:10:12Z",
        "submissionTimestamp": "2023-08-07T23:10:12Z",
        "subscriptionId": "5044AD67-CC18-4153-BE7E-CEFD01E28625",
        "tenantId": ""
      },
      {
        "channels": "Operation",
        "correlationId": "37abbfff-f5b3-476f-9040-20c6e29d4cf1",
        "description": "Backup operation started",
        "eventDataId": "c30358f3-2503-a504-46fc-364cdd8ba904",
        "eventName": {
          "value": "",
          "localizedValue": ""
        },
        "category": {
          "value": "Administrative",
          "localizedValue": "Administrative"
        },
        "id": "/SUBSCRIPTIONS/5044AD67-CC18-4153-BE7E-CEFD01E28625/RESOURCEGROUPS/CASAMENTORG/PROVIDERS/MICROSOFT.WEB/SITES/JOANAEPEDRO/events/c30358f3-2503-a504-46fc-364cdd8ba904/ticks/638270466020000000",
        "level": "Informational",
        "resourceGroupName": "CASAMENTORG",
        "resourceProviderName": {
          "value": "MICROSOFT.WEB",
          "localizedValue": "Azure Web Sites"
        },
        "resourceId": "/SUBSCRIPTIONS/5044AD67-CC18-4153-BE7E-CEFD01E28625/RESOURCEGROUPS/CASAMENTORG/PROVIDERS/MICROSOFT.WEB/SITES/JOANAEPEDRO",
        "resourceType": {
          "value": "MICROSOFT.WEB/sites",
          "localizedValue": "MICROSOFT.WEB/sites"
        },
        "operationId": "",
        "operationName": {
          "value": "BackupWebSite",
          "localizedValue": "BackupWebSite"
        },
        "properties": {
          "Message": "Backup operation started"
        },
        "status": {
          "value": "Started",
          "localizedValue": "Started"
        },
        "subStatus": {
          "value": "",
          "localizedValue": ""
        },
        "eventTimestamp": "2023-08-07T23:10:02Z",
        "submissionTimestamp": "2023-08-07T23:10:02Z",
        "subscriptionId": "5044AD67-CC18-4153-BE7E-CEFD01E28625",
        "tenantId": ""
      }
    ]